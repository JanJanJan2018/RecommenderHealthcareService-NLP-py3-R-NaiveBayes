---
title: "Massage Modality Recommender System"
author: "Janis Corona"
date: "4/27/2020"
output: html_document
---

This script uses 19 classes of massage therapy to recommend a massage for a client after excluding all other massage modalities based on the contraindications for each massage modality and benefit of each massage modality needed.

- [Random Forest Trees user input generated results](#random-forest-trees-user-input-generated-results)

- [Gradient Boosted Trees user input generated results](#gradient-boosted-trees-user-input-generated-results)

```{r}
modes <- read.csv('MassageModalities2.csv', sep=',', header=TRUE, na.strings=c('',' ','NA'))
colnames(modes)[1] <- 'modality'
head(modes)
```

Lets use python 3 to tokenize the contraindications into three adjacent word pairs, and to tokenize the the benefits into two adjacent word pairs using the ngrams tokenization method.
```{r}
library(reticulate)
```

```{r}
conda_list(conda = "auto") 

```


```{r}
use_condaenv(condaenv = "python36")

```

```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}
import pandas as pd 
import matplotlib.pyplot as plt 
from textblob import TextBlob 
import sklearn 
import numpy as np 
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer 
from sklearn.naive_bayes import MultinomialNB 
from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix 

import re
import string
import nltk 

np.random.seed(47) 
```

```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}
modes = pd.read_csv('MassageModalities2.csv', encoding = 'unicode_escape') 
print(modes.head())
print(modes.columns)
```

```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}
print(modes['modality'].unique())
```

```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}
import numpy as np

modes = modes.reindex(np.random.permutation(modes.index))

print(modes.head())
print(modes.tail())
```

```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}
modes.groupby('modality').describe()
```

```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}
stopwords = nltk.corpus.stopwords.words('english')
ps=nltk.PorterStemmer()
wn=nltk.WordNetLemmatizer()
```



```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}
def lemmatize(text):
    text="".join([word.lower() for word in text if word not in string.punctuation])
    tokens=re.split('\W+', text)
    text=" ".join([wn.lemmatize(word) for word in tokens if word not in stopwords])#unlisted with N-grams vectorization
    #text=[wn.lemmatize(word) for word in tokens if word not in stopwords]#when using count Vectorization its a list
    #or else single letters returned.
    return text
```


```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}
modes['lemmatizedBenefits']=modes['benefits'].apply(lambda x: lemmatize(x))
modes['lemmatizedContraindications']=modes['contraindications'].apply(lambda x: lemmatize(x))

```


```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}
modes.columns
```



```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}
modes.head()
```


```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}
from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test=train_test_split(modes[['lemmatizedContraindications','lemmatizedBenefits']],modes['modality'],test_size=0.15)

```

```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}
X_train.head()
```



```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}
from sklearn.feature_extraction.text import CountVectorizer
```


```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}
n_gram3_vect=CountVectorizer(ngram_range=(3,3))
n_gram2_vect=CountVectorizer(ngram_range=(2,2))

```


```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}
type(X_train['lemmatizedBenefits'])
X_train['lemmatizedBenefits'].head()
```

```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}
type(X_train['lemmatizedContraindications'])
X_train['lemmatizedContraindications'].head()
```



```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}
n_gram3_vect_fit=n_gram3_vect.fit(X_train['lemmatizedContraindications'])
n_gram2_vect_fit=n_gram2_vect.fit(X_train['lemmatizedBenefits'])


n_gram3_train=n_gram3_vect_fit.transform(X_train['lemmatizedContraindications'])
n_gram3_test=n_gram3_vect_fit.transform(X_test['lemmatizedContraindications'])
n_gram2_train=n_gram2_vect_fit.transform(X_train['lemmatizedBenefits'])
n_gram2_test=n_gram2_vect_fit.transform(X_test['lemmatizedBenefits'])

```


```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}
print(len(n_gram3_vect_fit.get_feature_names()))
Ngram3 = n_gram3_vect_fit.get_feature_names()
print(Ngram3)
```


```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}
print(len(n_gram2_vect_fit.get_feature_names()))
Ngram2 = n_gram2_vect_fit.get_feature_names()
print(type(Ngram2))
print(Ngram2)

```

```{python}
n_gram3_train_df=pd.concat([X_train[['lemmatizedContraindications','lemmatizedBenefits']].reset_index(drop=True),pd.DataFrame(n_gram3_train.toarray())],axis=1)

n_gram3_test_df=pd.concat([X_test[['lemmatizedContraindications','lemmatizedBenefits']].reset_index(drop=True),pd.DataFrame(n_gram3_test.toarray())],axis=1)

```


```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}
n_gram3_train_df.head()
```


```{python}
n_gram2_train_df=pd.concat([X_train[['lemmatizedContraindications','lemmatizedBenefits']].reset_index(drop=True),pd.DataFrame(n_gram2_train.toarray())],axis=1)

n_gram2_test_df=pd.concat([X_test[['lemmatizedContraindications','lemmatizedBenefits']].reset_index(drop=True),pd.DataFrame(n_gram2_test.toarray())],axis=1)

```


```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}
n_gram2_train_df.head()

```

```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}

n_gram2_train2=pd.DataFrame(n_gram2_train.toarray())
n_gram3_train3=pd.DataFrame(n_gram3_train.toarray())
n_gram2_test2=pd.DataFrame(n_gram2_test.toarray())
n_gram3_test3=pd.DataFrame(n_gram3_test.toarray())

n_gram3_train3.columns=Ngram3
n_gram2_train2.columns=Ngram2
n_gram3_test3.columns=Ngram3
n_gram2_test2.columns=Ngram2


n_gram_2_3_train_df=pd.concat([X_train[['lemmatizedContraindications','lemmatizedBenefits']].reset_index(drop=True),n_gram2_train2,n_gram3_train3],axis=1)

n_gram_2_3_test_df=pd.concat([X_test[['lemmatizedContraindications','lemmatizedBenefits']].reset_index(drop=True),n_gram2_test2,n_gram3_test3],axis=1)

```

```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}
ngram23Train = pd.concat([n_gram2_train2,n_gram3_train3],axis=1)
ngram23Test = pd.concat([n_gram2_test2,n_gram3_test3],axis=1)
ngram23Train.head()
```


```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}
n_gram_2_3_train_df.head()
```

Write this table of ngram tokens out to csv.
```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}
n_gram_2_3_test_df.to_csv('ngrams2_3_test.csv',index=False)
n_gram_2_3_train_df.to_csv('ngrams2_3_train.csv', index=False)
y_train.to_csv('y_train_ngrams23.csv', index=False)
y_test.to_csv('y_test_ngrams23.csv', index=False)
```

Lets read in this large file in RStudio, and combine the data into one table.
```{r}
ngrams23train <- read.csv('ngrams2_3_train.csv', sep=',', header=TRUE, 
                          na.strings=c('',' ','NA'))
ngrams23test <- read.csv('ngrams2_3_test.csv', sep=',', header=TRUE,
                         na.strings=c('',' ','NA'))
ytrain <- read.csv('y_train_ngrams23.csv', sep=',', header=FALSE,
                   na.strings=c('',' ','NA'))
colnames(ytrain) <- 'modality'
ytest <- read.csv('y_test_ngrams23.csv', sep=',', header=FALSE,
                  na.strings=c('',' ','NA'))
colnames(ytest) <- 'modality'

train <- cbind(ytrain,ngrams23train)
test <- cbind(ytest,ngrams23test)

ngrams23All <- rbind(train,test)

write.csv(ngrams23All,'lemmNgramsBenefits2Contraindications3.csv', row.names=FALSE)
```

We now have the lemmatized ngram tokens of 2 adjacent word pairs for our benefits and three adjacent word pairs for our contraindications saved to csv to use later or as needed for building our recommender system for a specific massage modality.

Lets get back to python, for machine learning using our previous models for the random forest classifier and the gradient boosted trees classifier. Lets use the combined tokens for the benefits and contraindications to see how well these trees do in classifying our massage modalities.
```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import precision_recall_fscore_support as score
import time
```


```{python}
rf=RandomForestClassifier(n_estimators=150, max_depth=None, n_jobs=-1)
start=time.time()
rf_model=rf.fit(ngram23Train,y_train)
end=time.time()
fit_time=(end-start)
fit_time
```


```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}
start=time.time()
y_pred=rf_model.predict(ngram23Test)
end=time.time()
pred_time=(end-start)
pred_time
```


```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}

prd = pd.DataFrame(y_pred)
prd.columns=['Predicted']

prd.index=y_test.index
pred=pd.concat([pd.DataFrame(prd),y_test],axis=1)
print(pred)

```


```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}
from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix 

print('accuracy', accuracy_score(y_test, y_pred))
print('confusion matrix\n', confusion_matrix(y_test, y_pred))
print('(row=expected, col=predicted)')

print(classification_report(y_test, y_pred))
```


```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}
gb=GradientBoostingClassifier(n_estimators=150,max_depth=11)
start=time.time()
gb_model=gb.fit(ngram23Train,y_train)
end=time.time()
fit_time=(end-start)
fit_time
```

```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}
start=time.time()
y_pred=gb_model.predict(ngram23Test)
end=time.time()
pred_time=(end-start)
pred_time
```


```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}
prd = pd.DataFrame(y_pred)
prd.columns=['Predicted']

prd.index=y_test.index
pred=pd.concat([pd.DataFrame(prd),y_test],axis=1)
print(pred)
```


```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}
from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix 

print('accuracy', accuracy_score(y_test, y_pred))
print('confusion matrix\n', confusion_matrix(y_test, y_pred))
print('(row=expected, col=predicted)')

print(classification_report(y_test, y_pred))
```

It is great that these two produced the same results of 100%, as they should because each class of modality is a duplicate up to 23 duplicates, or 24 samples of each modality that are all identical. I ran a previous script on the same data and used 1-4 ngrams and the hot stone therapy observations were all getting misclassified as deep tissue recommendations for benefits and the same for contraindications of each type.

Lets try user inputs using this data after we make the above into a function for both models.
```{python}

def lemmatize(text):
    text="".join([word.lower() for word in text if word not in string.punctuation])
    tokens=re.split('\W+', text)
    text=" ".join([wn.lemmatize(word) for word in tokens if word not in stopwords])
    return text
    
def predict_ngramRFC_lemma(new_review): 
    nr=pd.DataFrame([new_review])
    nr.columns=['newReview']
    nr['lemma']=nr['newReview'].apply(lambda x: lemmatize(x))
    
    rf=RandomForestClassifier(n_estimators=150, max_depth=None, n_jobs=-1)
    n_gram2_vect=CountVectorizer(ngram_range=(2,2))
    n_gram3_vect=CountVectorizer(ngram_range=(3,3))


    n_gram2_vect_fit=n_gram2_vect.fit(X_train['lemmatizedBenefits'])
    n_gram3_vect_fit=n_gram3_vect.fit(X_train['lemmatizedContraindications'])


    n_gram2_train=n_gram2_vect_fit.transform(X_train['lemmatizedBenefits'])
    n_gram3_train=n_gram3_vect_fit.transform(X_train['lemmatizedContraindications'])

    Ngram2 = n_gram2_vect_fit.get_feature_names()
    Ngram3 = n_gram3_vect_fit.get_feature_names()

    n_gram2_train2=pd.DataFrame(n_gram2_train.toarray())
    n_gram3_train3=pd.DataFrame(n_gram3_train.toarray())

    n_gram2_train2.columns=Ngram2
    n_gram3_train3.columns=Ngram3

    ngram23Train = pd.concat([n_gram2_train2,n_gram3_train3],axis=1)

    nr_gram2_test=n_gram2_vect_fit.transform(nr['lemma'])
    nr_gram3_test=n_gram3_vect_fit.transform(nr['lemma'])
   
    nr_test2=pd.DataFrame(nr_gram2_test.toarray())
    nr_test3=pd.DataFrame(nr_gram3_test.toarray())
    
    nr_test2.columns=Ngram2
    nr_test3.columns=Ngram3

    nrTest = pd.concat([nr_test2,nr_test3],axis=1)
    
    model = rf.fit(ngram23Train,y_train)
    pred=pd.DataFrame(model.predict(nrTest))
    pred.columns=['Recommended Healthcare Service:']
    pred.index= ['lemmatized_2ngram3_RFC_85-15:']
    print('\n\n',pred)
```

```{python} 
np.random.seed(12345)
predict_ngramRFC_lemma('I need a massage!') 
```


```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}
np.random.seed(12345)

predict_ngramRFC_lemma('I have been working out a lot more than normal and am sore all over. Feels like a car hit me. I can\'t touch my toes to tie my shoes and my neck won\'t turn to the right. Help me.')

```

```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}

def lemmatize(text):
    text="".join([word.lower() for word in text if word not in string.punctuation])
    tokens=re.split('\W+', text)
    text=" ".join([wn.lemmatize(word) for word in tokens if word not in stopwords])
    return text
    

def predict_ngramGBC_lemma(new_review): 
    nr=pd.DataFrame([new_review])
    nr.columns=['newReview']
    nr['lemma']=nr['newReview'].apply(lambda x: lemmatize(x))

    gb=GradientBoostingClassifier(n_estimators=150,max_depth=11)
    n_gram2_vect=CountVectorizer(ngram_range=(2,2))
    n_gram3_vect=CountVectorizer(ngram_range=(3,3))


    n_gram2_vect_fit=n_gram2_vect.fit(X_train['lemmatizedBenefits'])
    n_gram3_vect_fit=n_gram3_vect.fit(X_train['lemmatizedContraindications'])


    n_gram2_train=n_gram2_vect_fit.transform(X_train['lemmatizedBenefits'])
    n_gram3_train=n_gram3_vect_fit.transform(X_train['lemmatizedContraindications'])

    Ngram2 = n_gram2_vect_fit.get_feature_names()
    Ngram3 = n_gram3_vect_fit.get_feature_names()

    n_gram2_train2=pd.DataFrame(n_gram2_train.toarray())
    n_gram3_train3=pd.DataFrame(n_gram3_train.toarray())

    n_gram2_train2.columns=Ngram2
    n_gram3_train3.columns=Ngram3


    ngram23Train = pd.concat([n_gram2_train2,n_gram3_train3],axis=1)

    nr_gram2_test=n_gram2_vect_fit.transform(nr['lemma'])
    nr_gram3_test=n_gram3_vect_fit.transform(nr['lemma'])
   
    nr_test2=pd.DataFrame(nr_gram2_test.toarray())
    nr_test3=pd.DataFrame(nr_gram3_test.toarray())
    
    nr_test2.columns=Ngram2
    nr_test3.columns=Ngram3

    nrTest = pd.concat([nr_test2,nr_test3],axis=1)

    model = gb.fit(ngram23Train,y_train)
    pred=pd.DataFrame(model.predict(nrTest))
    pred.columns=['Recommended Healthcare Service:']
    pred.index= ['lemmatized_2ngram3_GBC_85-15:']
    print('\n\n',pred)
```

```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}    
np.random.seed(12345)

predict_ngramGBC_lemma('I need a massage!')
```


```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}
np.random.seed(12345)

predict_ngramGBC_lemma('I have been working out a lot more than normal and am sore all over. Feels like a car hit me. I can\'t touch my toes to tie my shoes and my neck won\'t turn to the right. Help me.')

```

That was pretty interesting, to see the different recommendations. Since many of the contraindications and benefits are the same between modalities, these simple user inputs produced the same results with the seed set. If I remove the seed or starting point to randomize within the operating system, then lets see how this knits.


# Gradient Boosted Trees user input generated results

```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}    
predict_ngramGBC_lemma('I need a massage!')
```


```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}    
predict_ngramGBC_lemma('I need a massage!')
```


```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}    
predict_ngramGBC_lemma('I need a massage!')
```


```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}    
predict_ngramGBC_lemma('I need a massage!')
```


```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}    
predict_ngramGBC_lemma('I need a massage!')
```

```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}
predict_ngramGBC_lemma('I have been working out a lot more than normal and am sore all over. Feels like a car hit me. I can\'t touch my toes to tie my shoes and my neck won\'t turn to the right. Help me.')

```

```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}
predict_ngramGBC_lemma('I have been working out a lot more than normal and am sore all over. Feels like a car hit me. I can\'t touch my toes to tie my shoes and my neck won\'t turn to the right. Help me.')

```

```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}
predict_ngramGBC_lemma('I have been working out a lot more than normal and am sore all over. Feels like a car hit me. I can\'t touch my toes to tie my shoes and my neck won\'t turn to the right. Help me.')

```

```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}
predict_ngramGBC_lemma('I have been working out a lot more than normal and am sore all over. Feels like a car hit me. I can\'t touch my toes to tie my shoes and my neck won\'t turn to the right. Help me.')

```

```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}
predict_ngramGBC_lemma('I have been working out a lot more than normal and am sore all over. Feels like a car hit me. I can\'t touch my toes to tie my shoes and my neck won\'t turn to the right. Help me.')

```

```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}
predict_ngramGBC_lemma('I have been working out a lot more than normal and am sore all over. Feels like a car hit me. I can\'t touch my toes to tie my shoes and my neck won\'t turn to the right. Help me.')

```

```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}
predict_ngramGBC_lemma('I have been working out a lot more than normal and am sore all over. Feels like a car hit me. I can\'t touch my toes to tie my shoes and my neck won\'t turn to the right. Help me.')

```

```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}
predict_ngramGBC_lemma('I have been working out a lot more than normal and am sore all over. Feels like a car hit me. I can\'t touch my toes to tie my shoes and my neck won\'t turn to the right. Help me.')

```

# Random Forest Trees user input generated results
```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}    
predict_ngramRFC_lemma('I need a massage!')
```

```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}    
predict_ngramRFC_lemma('I need a massage!')
```

```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}    
predict_ngramRFC_lemma('I need a massage!')
```

```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}    
predict_ngramRFC_lemma('I need a massage!')
```

```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}    
predict_ngramRFC_lemma('I need a massage!')
```

```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}    
predict_ngramRFC_lemma('I need a massage!')
```


```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}
predict_ngramRFC_lemma('I have been working out a lot more than normal and am sore all over. Feels like a car hit me. I can\'t touch my toes to tie my shoes and my neck won\'t turn to the right. Help me.')

```

```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}
predict_ngramRFC_lemma('I have been working out a lot more than normal and am sore all over. Feels like a car hit me. I can\'t touch my toes to tie my shoes and my neck won\'t turn to the right. Help me.')

```

```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}
predict_ngramRFC_lemma('I have been working out a lot more than normal and am sore all over. Feels like a car hit me. I can\'t touch my toes to tie my shoes and my neck won\'t turn to the right. Help me.')

```

```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}
predict_ngramRFC_lemma('I have been working out a lot more than normal and am sore all over. Feels like a car hit me. I can\'t touch my toes to tie my shoes and my neck won\'t turn to the right. Help me.')

```

```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}
predict_ngramRFC_lemma('I have been working out a lot more than normal and am sore all over. Feels like a car hit me. I can\'t touch my toes to tie my shoes and my neck won\'t turn to the right. Help me.')

```

```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}
predict_ngramRFC_lemma('I have been working out a lot more than normal and am sore all over. Feels like a car hit me. I can\'t touch my toes to tie my shoes and my neck won\'t turn to the right. Help me.')

```


Wonderful! But now lets try to get some of the modalities other than CBD, biofreeze, aromatherapy, stretching, and lymphatic drainage massage. Those are more additional therapeutics for massage therapy.

```{python,error=FALSE,message=FALSE,warning=FALSE,FutureWarning=FALSE}
predict_ngramGBC_lemma('I want either a Swedish or Deep Tissue massage, I want a lot of pressure, and need to fall asleep, I workout, have stress at work, alright with some hot stones or cold stones, or added cups.')
```



Our next model will use ngrams on the modality description to better capture the tokenized words for each modality, and keep the bigrams on benefits and trigrams on contraindications.Also, there should be a filter system so that those contraindicated massage modalities are excluded from the next run using the benefits or expectations of a user. These choices are selecting both benefits and those massages contraindicated with the current design. This did make the prediction on the testing set 100% accurate as it should, because only using benefits or contraindications tokenized produced a precision error on classifying hot stone therapy as deep tissue massage from a previous script done in Jupyter Notebook for python.


***

Lets switch to R.
```{r}
ngrams23All <- read.csv('lemmNgramsBenefits2Contraindications3.csv', sep=',', header=TRUE,
                        na.strings=c('',' ','NA'))
```

```{r}
unique(ngrams23All$modality)

```

```{r}
myofascial <- subset(ngrams23All, ngrams23All$modality=='Myofascial Massage')
dim(myofascial)
```

```{r}
myofascial1 <- myofascial[,-c(1:3)]
myofascial2 <- subset(myofascial1, colSums(myofascial1)!=0)
dim(myofascial2)
```
```{r}
myofascial2 <- myofascial1[,colSums(myofascial1) >= 1]
colSums(myofascial2)
```

```{r}
contraMyo <- grep('[.].*[.].*',colnames(myofascial2))
myoContra <- myofascial2[,contraMyo]
myoBenefit <- myofascial2[,-contraMyo]
```

We now have the benefits as bigrams and contraindications as trigrams to 
exclude this as a list of the myofascial therapy contraindications, and to 
recommend the list of myofascial benefits. We just have to make these column names
into lists for benefits and contraindications.
```{r}
benefits_myofascial <- gsub('[.]',' ', colnames(myoBenefit), perl=TRUE)
contra_myofascial <- gsub('[.]',' ', colnames(myoContra), perl=TRUE)
```

We will make the lists of the other 18 categories of massage modalities' benefits and contraindications to use in building our recommender system for massage modalities for each user.

Prenatal Massage
```{r}
prenatal <- subset(ngrams23All, ngrams23All$modality=='Prenatal Massage')

prenatal1 <- prenatal[,-c(1:3)]
prenatal2 <- subset(prenatal1, colSums(prenatal1)!=0)

prenatal2 <- prenatal1[,colSums(prenatal1) >= 1]

contraPre <- grep('[.].*[.].*',colnames(prenatal2))
PreContra <- prenatal2[,contraPre]
PreBenefit <- prenatal2[,-contraPre]

benefits_prenatal <- gsub('[.]',' ', colnames(PreBenefit), perl=TRUE)
contra_prenatal <- gsub('[.]',' ', colnames(PreContra), perl=TRUE)

```

Shiatsu Massage
```{r}
shiatsu <- subset(ngrams23All, ngrams23All$modality=='Shiatsu Massage')

shiatsu1 <- shiatsu[,-c(1:3)]
shiatsu2 <- subset(shiatsu1, colSums(shiatsu1)!=0)

shiatsu2 <- shiatsu1[,colSums(shiatsu1) >= 1]

contrashi <- grep('[.].*[.].*',colnames(shiatsu2))
shiContra <- shiatsu2[,contrashi]
shiBenefit <- shiatsu2[,-contrashi]

benefits_shiatsu <- gsub('[.]',' ', colnames(shiBenefit), perl=TRUE)
contra_shiatsu <- gsub('[.]',' ', colnames(shiContra), perl=TRUE)

```

Hot Stone Therapy Massage
```{r}
hotStone <- subset(ngrams23All, ngrams23All$modality=='Hot Stone Therapy Massage')

hotStone1 <- hotStone[,-c(1:3)]
hotStone2 <- subset(hotStone1, colSums(hotStone1)!=0)

hotStone2 <- hotStone1[,colSums(hotStone1) >= 1]

contrahs <- grep('[.].*[.].*',colnames(hotStone2))
hsContra <- hotStone2[,contrahs]
hsBenefit <- hotStone2[,-contrahs]

benefits_hs <- gsub('[.]',' ', colnames(hsBenefit), perl=TRUE)
contra_hs <- gsub('[.]',' ', colnames(hsContra), perl=TRUE)
```

Cupping Therapy
```{r}
Cupping <- subset(ngrams23All, ngrams23All$modality=='Cupping Therapy')

Cupping1 <- Cupping[,-c(1:3)]
Cupping2 <- subset(Cupping1, colSums(Cupping1)!=0)

Cupping2 <- Cupping1[,colSums(Cupping1) >= 1]

contracup <- grep('[.].*[.].*',colnames(Cupping2))
cupContra <- Cupping2[,contracup]
cupBenefit <- Cupping2[,-contracup]

benefits_cup <- gsub('[.]',' ', colnames(cupBenefit), perl=TRUE)
contra_cup <- gsub('[.]',' ', colnames(cupContra), perl=TRUE)
```

Sports Massage
```{r}
Sports <- subset(ngrams23All, ngrams23All$modality=='Sports Massage')

Sports1 <- Sports[,-c(1:3)]
Sports2 <- subset(Sports1, colSums(Sports1)!=0)

Sports2 <- Sports1[,colSums(Sports1) >= 1]

contrasports <- grep('[.].*[.].*',colnames(Sports2))
sportsContra <- Sports2[,contrasports]
sportsBenefit <- Sports2[,-contrasports]

benefits_sports <- gsub('[.]',' ', colnames(sportsBenefit), perl=TRUE)
contra_sports <- gsub('[.]',' ', colnames(sportsContra), perl=TRUE)

```

Biofreeze Muscle Pain Relief Gel
```{r}
Freeze <- subset(ngrams23All, ngrams23All$modality=='Biofreeze Muscle Pain Relief Gel')

Freeze1 <- Freeze[,-c(1:3)]
Freeze2 <- subset(Freeze1, colSums(Freeze1)!=0)

Freeze2 <- Freeze1[,colSums(Freeze1) >= 1]

contrafreeze <- grep('[.].*[.].*',colnames(Freeze2))
freezeContra <- Freeze2[,contrafreeze]
freezeBenefit <- Freeze2[,-contrafreeze]

benefits_freeze <- gsub('[.]',' ', colnames(freezeBenefit), perl=TRUE)
contra_freeze <- gsub('[.]',' ', colnames(freezeContra), perl=TRUE)

```

Cold Stone Therapy
```{r}
ColdStone <- subset(ngrams23All, ngrams23All$modality=='Cold Stone Therapy')

ColdStone1 <- ColdStone[,-c(1:3)]
ColdStone2 <- subset(ColdStone1, colSums(ColdStone1)!=0)

ColdStone2 <- ColdStone1[,colSums(ColdStone1) >= 1]

contracold <- grep('[.].*[.].*',colnames(ColdStone2))
coldContra <- ColdStone2[,contracold]
coldBenefit <- ColdStone2[,-contracold]

benefits_cold <- gsub('[.]',' ', colnames(coldBenefit), perl=TRUE)
contra_cold <- gsub('[.]',' ', colnames(coldContra), perl=TRUE)

```

Stretching
```{r}
Stretch <- subset(ngrams23All, ngrams23All$modality=='Stretching')

Stretch1 <- Stretch[,-c(1:3)]
Stretch2 <- subset(Stretch1, colSums(Stretch1)!=0)

Stretch2 <- Stretch1[,colSums(Stretch1) >= 1]

contrastretch <- grep('[.].*[.].*',colnames(Stretch2))
stretchContra <- Stretch2[,contrastretch]
stretchBenefit <- Stretch2[,-contrastretch]

benefits_stretch <- gsub('[.]',' ', colnames(stretchBenefit), perl=TRUE)
contra_stretch <- gsub('[.]',' ', colnames(stretchContra), perl=TRUE)

```

Aromatherapy
```{r}
AromaTherapy <- subset(ngrams23All, ngrams23All$modality=='Aromatherapy')

AromaTherapy1 <- AromaTherapy[,-c(1:3)]
AromaTherapy2 <- subset(AromaTherapy1, colSums(AromaTherapy1)!=0)

AromaTherapy2 <- AromaTherapy1[,colSums(AromaTherapy1) >= 1]

contraaroma <- grep('[.].*[.].*',colnames(AromaTherapy2))
aromaContra <- AromaTherapy2[,contraaroma]
aromaBenefit <- AromaTherapy2[,-contraaroma]

benefits_aroma <- gsub('[.]',' ', colnames(aromaBenefit), perl=TRUE)
contra_aroma <- gsub('[.]',' ', colnames(aromaContra), perl=TRUE)

```

Swedish Massage
```{r}
Swedish <- subset(ngrams23All, ngrams23All$modality=='Swedish Massage')

Swedish1 <- Swedish[,-c(1:3)]
Swedish2 <- subset(Swedish1, colSums(Swedish1)!=0)

Swedish2 <- Swedish1[,colSums(Swedish1) >= 1]

contraswedish <- grep('[.].*[.].*',colnames(Swedish2))
swedishContra <- Swedish2[,contraswedish]
swedishBenefit <- Swedish2[,-contraswedish]

benefits_swedish <- gsub('[.]',' ', colnames(swedishBenefit), perl=TRUE)
contra_swedish <- gsub('[.]',' ', colnames(swedishContra), perl=TRUE)

```

Deep tissue Massage
```{r}
DTmassage <- subset(ngrams23All, ngrams23All$modality=='Deep tissue Massage')

DTmassage1 <- DTmassage[,-c(1:3)]
DTmassage2 <- subset(DTmassage1, colSums(DTmassage1)!=0)

DTmassage2 <- DTmassage1[,colSums(DTmassage1) >= 1]

contraDT <- grep('[.].*[.].*',colnames(DTmassage2))
DTContra <- DTmassage2[,contraDT]
DTBenefit <- DTmassage2[,-contraDT]

benefits_DT <- gsub('[.]',' ', colnames(DTBenefit), perl=TRUE)
contra_DT <- gsub('[.]',' ', colnames(DTContra), perl=TRUE)

```

Instrument Assisted Soft Tissue Mobilization (IASTM) Friction Massage
```{r}
IASTM <- subset(ngrams23All, ngrams23All$modality=='Instrument Assisted Soft Tissue Mobilization (IASTM) Friction Massage')

IASTM1 <- IASTM[,-c(1:3)]
IASTM2 <- subset(IASTM1, colSums(IASTM1)!=0)

IASTM2 <- IASTM1[,colSums(IASTM1) >= 1]

contrainstrument <- grep('[.].*[.].*',colnames(IASTM2))
instrumentContra <- IASTM2[,contrainstrument]
instrumentBenefit <- IASTM2[,-contrainstrument]

benefits_instrument <- gsub('[.]',' ', colnames(instrumentBenefit), perl=TRUE)
contra_instrument <- gsub('[.]',' ', colnames(instrumentContra), perl=TRUE)

```

Trigger Point Therapy
```{r}
TPT <- subset(ngrams23All, ngrams23All$modality=='Trigger Point Therapy')

TPT1 <- TPT[,-c(1:3)]
TPT2 <- subset(TPT1, colSums(TPT1)!=0)

TPT2 <- TPT1[,colSums(TPT1) >= 1]

contratpt <- grep('[.].*[.].*',colnames(TPT2))
tptContra <- TPT2[,contratpt]
tptBenefit <- TPT2[,-contratpt]

benefits_tpt <- gsub('[.]',' ', colnames(tptBenefit), perl=TRUE)
contra_tpt <- gsub('[.]',' ', colnames(tptContra), perl=TRUE)

```

Massage Gun Therapy
```{r}
massageGun <- subset(ngrams23All, ngrams23All$modality=='Massage Gun Therapy')

massageGun1 <- massageGun[,-c(1:3)]
massageGun2 <- subset(massageGun1, colSums(massageGun1)!=0)

massageGun2 <- massageGun1[,colSums(massageGun1) >= 1]

contramassagegun <- grep('[.].*[.].*',colnames(massageGun2))
massagegunContra <- massageGun2[,contramassagegun]
massagegunBenefit <- massageGun2[,-contramassagegun]

benefits_massagegun <- gsub('[.]',' ', colnames(massagegunBenefit), perl=TRUE)
contra_massagegun <- gsub('[.]',' ', colnames(massagegunContra), perl=TRUE)

```

Lymphatic Drainage Massage
```{r}
Lymphatic <- subset(ngrams23All, ngrams23All$modality=='Lymphatic Drainage Massage')

Lymphatic1 <- Lymphatic[,-c(1:3)]
Lymphatic2 <- subset(Lymphatic1, colSums(Lymphatic1)!=0)

Lymphatic2 <- Lymphatic1[,colSums(Lymphatic1) >= 1]

contralymphatic <- grep('[.].*[.].*',colnames(Lymphatic2))
lymphaticContra <- Lymphatic2[,contralymphatic]
lymphaticBenefit <- Lymphatic2[,-contralymphatic]

benefits_lymphatic <- gsub('[.]',' ', colnames(lymphaticBenefit), perl=TRUE)
contra_lymphatic <- gsub('[.]',' ', colnames(lymphaticContra), perl=TRUE)

```

Reflexology Massage
```{r}
Reflexology <- subset(ngrams23All, ngrams23All$modality=='Reflexology Massage')

Reflexology1 <- Reflexology[,-c(1:3)]
Reflexology2 <- subset(Reflexology1, colSums(Reflexology1)!=0)

Reflexology2 <- Reflexology1[,colSums(Reflexology1) >= 1]

contrareflexology <- grep('[.].*[.].*',colnames(Reflexology2))
reflexologyContra <- Reflexology2[,contrareflexology]
reflexologyBenefit <- Reflexology2[,-contrareflexology]

benefits_reflexology <- gsub('[.]',' ', colnames(reflexologyBenefit), perl=TRUE)
contra_reflexology <- gsub('[.]',' ', colnames(reflexologyContra), perl=TRUE)

```

Craniosacral Massage
```{r}
Craniosacral <- subset(ngrams23All, ngrams23All$modality=='Craniosacral Massage')

Craniosacral1 <- Craniosacral[,-c(1:3)]
Craniosacral2 <- subset(Craniosacral1, colSums(Craniosacral1)!=0)

Craniosacral2 <- Craniosacral1[,colSums(Craniosacral1) >= 1]

contracraniosacral <- grep('[.].*[.].*',colnames(Craniosacral2))
craniosacralContra <- Craniosacral2[,contracraniosacral]
craniosacralBenefit <- Craniosacral2[,-contracraniosacral]

benefits_craniosacral <- gsub('[.]',' ', colnames(craniosacralBenefit), perl=TRUE)
contra_craniosacral <- gsub('[.]',' ', colnames(craniosacralContra), perl=TRUE)

```

Cannabidiol (CBD) Massage Balm
```{r}
CBD <- subset(ngrams23All, ngrams23All$modality=='Cannabidiol (CBD) Massage Balm')

CBD1 <- CBD[,-c(1:3)]
CBD2 <- subset(CBD1, colSums(CBD1)!=0)

CBD2 <- CBD1[,colSums(CBD1) >= 1]

contracbd <- grep('[.].*[.].*',colnames(CBD2))
cbdContra <- CBD2[,contracbd]
cbdBenefit <- CBD2[,-contracbd]

benefits_cbd <- gsub('[.]',' ', colnames(cbdBenefit), perl=TRUE)
contra_cbd <- gsub('[.]',' ', colnames(cbdContra), perl=TRUE)

```

Benefits of each modality as lists:
benefits_cbd,benefits_craniosacral,benefits_reflexology,benefits_lymphatic,benefits_massagegun,benefits_tpt,benefits_instrument,benefits_DT,benefits_swedish,benefits_aroma,benefits_stretch,benefits_cold,benefits_freeze,benefits_sports,benefits_cup,benefits_hs,benefits_shiatsu,benefits_prenatal,benefits_myofascial

Contraindications of each modality as lists:
contra_cbd,contra_craniosacral,contra_reflexology,contra_lymphatic,contra_massagegun,contra_tpt,contra_instrument,contra_DT,contra_swedish,contra_aroma,contra_stretch,contra_cold,contra_freeze,
contra_sports,contra_cup,contra_hs,contra_shiatsu,contra_prenatal,contra_myofascial

```{r}
benefits_cbd
contra_cbd
```

The above demonstrates the benefits of CBD as a list of double word pairs or bigrams with the stop words stripped, and the bottom list is the longer list of trigrams of three word groups for the contraindications for CBD with the stopwords stripped. What we now want is a way to get the user the have a user input that will scan the list of contraindications for each massage modality, and if it is in the list of a modality, then it will be excluded from the list of available massage modalities for the user. 

Looking at the above list the contraindications are grouped together that are different health conditions like psychosis (and) numbness (in the) limb. Users don't want to scan a list of 500 health conditions or even more than 10, but any health conditions they have will have to be reported before scheduling a massage so that it isn't cancelled or booked for the wrong modality. Lets assume the user honestly includes every possible health condition and history of their health conditions for serious medical conditions, then we want this program to scan those groups of words and find the modalities the user absolutely should not have, so that a list of available massage modalities are provided for the user to select the best one for benefits.

